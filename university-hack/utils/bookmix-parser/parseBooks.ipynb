{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "from skimage.io import imread\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBALS\n",
    "\n",
    "URL_BOOXMIX = \"https://bookmix.ru\"\n",
    "OPTIONS_BOOXMIX = \"/reviews.phtml?option=all&begin=%d&num_point=10&num_points=10\"\n",
    "\n",
    "CSV_COLUMNS=[\"name\", \"author\", \"description\", \"marks\", \"review\", \"image\", \"height\", \"url_to_book\", \"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_news(parser):\n",
    "    global BOOKS_CSV\n",
    "    \"\"\" Extract news from a given web page \"\"\"\n",
    "    news_list = []\n",
    "    \n",
    "    books_list = parser.find(\"ul\", {\"class\": \"books_list\"})\n",
    "    news_divs = books_list.find_all(\"li\")\n",
    "    \n",
    "    for ind in range(len(news_divs)):\n",
    "        try:\n",
    "            url_to_review = news_divs[ind].find(\"div\", {\"class\" : \"inner\"}).find(\"h4\").a[\"href\"]\n",
    "            url_to_book = news_divs[ind].find(\"div\", {\"class\" : \"inner\"}).find(\"div\", {\"class\" : \"authors\"}).span.a[\"href\"]\n",
    "            \n",
    "            review_soup = get_soup(URL_BOOXMIX + url_to_review)\n",
    "            book_soup   = get_soup(URL_BOOXMIX + url_to_book)\n",
    "            \n",
    "            name = news_divs[ind].find(\"div\", {\"class\" : \"inner\"}).find(\"h4\").text\n",
    "            author = news_divs[ind].find_all(\"div\", {\"class\" : \"authors\"})[-1].span.text.replace(\"\\n\", \"\")\n",
    "            description = extract_description(book_soup)\n",
    "            marks  = extract_marks(book_soup)\n",
    "            review = extract_review(review_soup).replace(\"\\r\", \"\").replace(\"\\n\", \"\")\n",
    "            image  = extract_image(book_soup)\n",
    "            \n",
    "            height = imread(image).shape[0]\n",
    "            \n",
    "            news = pd.Series([name, author, description, marks, review, image, height, URL_BOOXMIX + url_to_book, None], index=CSV_COLUMNS).to_frame().T\n",
    "\n",
    "            BOOKS_CSV = BOOKS_CSV.append(news, ignore_index=True)\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "\n",
    "        \n",
    "def get_soup(url):\n",
    "    response = requests.get(url)\n",
    "    response.encoding = response.apparent_encoding\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    return soup\n",
    "\n",
    "\n",
    "def extract_review(soup):\n",
    "    review_content = soup.find(\"div\", {\"class\": \"comment-text\"}).text\n",
    "    for p in soup.find_all(\"p\", {\"class\": \"br\"}): review_content += p.text\n",
    "    \n",
    "    return review_content\n",
    "    \n",
    "\n",
    "def extract_image(soup):\n",
    "    image = soup.find(\"div\", {\"class\": \"thumb\"}).img[\"src\"]\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def extract_description(soup):\n",
    "    description = soup.find(\"p\", {\"itemprop\": \"about\"}).text\n",
    "\n",
    "    return description\n",
    "\n",
    "\n",
    "def extract_marks(soup):\n",
    "    marks = \"\"\n",
    "    \n",
    "    for keywords in soup.find_all(\"div\", {\"itemprop\": \"keywords\"}):\n",
    "        for a in keywords.find_all(\"a\"):\n",
    "            marks += a.text + \";\"\n",
    "    \n",
    "    return marks\n",
    "\n",
    "\n",
    "def get_news_from_bookmix(n_pages=1):\n",
    "    \"\"\" Collect news from a given web page \"\"\"\n",
    "    news = []\n",
    "\n",
    "    while n_pages:\n",
    "        response = requests.get(URL_BOOXMIX + OPTIONS_BOOXMIX % (10 * n_pages))\n",
    "        response.encoding = response.apparent_encoding\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        \n",
    "        extract_news(soup)\n",
    "        n_pages -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOOKS_CSV = pd.DataFrame()\n",
    "get_news_from_bookmix(100)\n",
    "\n",
    "BOOKS_CSV.to_csv(\"books.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs102",
   "language": "python",
   "name": "cs102"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
